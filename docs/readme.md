# openCOSMOâ€‘RS Pipeline

A fully reproducible, deterministic, and extensible scientific workflow engine for:

- Molecular cleaning & metadata generation  
- Conformer generation  
- Conformer pruning  
- Geometry optimisation (XTB, gXTB, ORCA)  
- ORCA COSMO calculations  
- COSMOâ€‘RS solubility prediction  

The pipeline is designed for **robustness**, **traceability**, and **scientific reproducibility**, with:

- Canonical stage inputs/outputs  
- Deterministic job chaining  
- Full provenance tracking  
- Resume & continuation support  
- Configâ€‘driven execution  
- Git version embedding  

---

## ğŸš€ Quickstart

### 1. Prepare your environment

Install dependencies (RDKit, ORCA, XTB, gXTB, CREST, COSMOâ€‘RS bindings).  
See `docs/installation.md` for full details.

### 2. Configure paths

Edit:

```
config/paths.json
```

to point to:

- ORCA executable  
- XTB/gXTB executables  
- CREST  
- COSMOâ€‘RS Python + C++ bindings  
- CONSTANT_FILES directory  
- pipeline_data directory  

### 3. Run the pipeline

```bash
python3 -m modules.main
```

This will:

- Create a new Request  
- Create the first Job  
- Execute each stage sequentially  
- Produce a full provenance trail  

### 4. Resume a request

If a job was interrupted (Ctrl+C, crash, HPC preemption):

```bash
python3 -m modules.main_resume
```

The pipeline will:

- Skip completed stages  
- Resume the interrupted job  
- Continue normally  

### 5. Continue from a previous request

To start a new pipeline using the output of a previous job:

```bash
python3 -m modules.main_continue
```

---

## ğŸ“‚ Directory Structure

```
pipeline_data/
    requests/
        R-<timestamp>-<title>/
            request.json
            pipeline_state.json
            request.log
            jobs/
                J-<timestamp>-<stage>/
                    inputs/
                    outputs/
                    job_state.json
                    stage.log
```

### Key files:

- **request.json** â€” immutable record of user intent  
- **pipeline_state.json** â€” current stage, last completed stage  
- **job_state.json** â€” itemâ€‘level progress (pending/completed/failed)  
- **stage.log** â€” detailed execution log  
- **canonical outputs** â€” e.g., `cleaned.csv`, `energies.json`, `orcacosmo_summary.json`  

---

## ğŸ§  Pipeline Stages

### 1. Cleaning

- Reads raw CSVs  
- Standardises headers  
- Canonicalises SMILES  
- Generates InChIKeys  
- Computes physchem descriptors  
- Writes molecule metadata (local + global)  
- Output: `cleaned.csv`

### 2. Generation

- Generates conformers (RDKit or CREST)  
- Output: `energies.json`

### 3. Pruning

- Selects topâ€‘N conformers  
- Output: `energies.json`

### 4. Optimisation

- Runs ORCA, XTB, or gXTB  
- Checkpointed  
- Fully resumable  
- Output: `energies.json`

### 5. ORCA COSMO

- Writes ORCA input files  
- Runs TZVPD â†’ fallback TZVP  
- Parses log/cpcm/cpcm_corr  
- Reconstructs `.orcacosmo` files  
- Output: `orcacosmo_summary.json`

### 6. Solubility

- Runs COSMOâ€‘RS  
- Output: `solubility_results.json`

---

## ğŸ” Resume & Continuation

### Resume (same request)

If a job is interrupted:

- `job_state.json` tracks pending items  
- `pipeline_state.json` tracks current stage  

Resume with:

```bash
python3 -m modules.main_resume
```

### Continue (new request)

To start a new pipeline using the output of a previous job:

```bash
python3 -m modules.main_continue
```

This creates a new Request with:

- New request ID  
- New job lineage  
- Stage 0 input = previous jobâ€™s canonical output  

---

## ğŸ§¬ Provenance & Reproducibility

Every metadata file includes:

- Git version (`main@abc1234`)  
- Cleaning timestamp  
- Request ID  
- Job ID  
- Source file  
- Pipeline version  

Every stage:

- Has a canonical input  
- Has a canonical output  
- Never guesses or autoâ€‘detects  
- Is deterministic  

---

## ğŸ›  Adding a New Stage

See `docs/developer_guide.md` for full details.

In short:

1. Create `modules/stages/<name>_stage.py`
2. Subclass `BaseStage`
3. Implement:
   - `execute()`
   - `set_stage_output()`
   - `require_file()`
4. Add canonical output to `Job.STAGE_OUTPUTS`
5. Add stage to pipeline spec

---

## ğŸ§ª Example

```python
pipeline_spec = [
    {"stage": "cleaning", "args": {"input_csv": "data.csv"}},
    {"stage": "generation", "args": {"engine": "rdkit"}},
    {"stage": "pruning", "args": {"n": 1}},
    {"stage": "optimisation", "args": {"engine": "xtb_opt_normal"}},
    {"stage": "orcacosmo", "args": {}},
    {"stage": "solubility", "args": {}},
]
```

---

## ğŸ“„ License

MIT or your preferred license.

---

## ğŸ‘¤ Authors

Luke Hetherington  
(openCOSMOâ€‘RS Pipeline Architect)

---

